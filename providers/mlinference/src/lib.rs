#![allow(dead_code)]
use thiserror::Error as ThisError;

use std::{
    collections::{HashMap},
};
use serde::{Deserialize};
use wasmcloud_interface_mlinference::{ ResultStatus, MlError, InferenceOutput, TensorOut };

mod bindle_loader;
pub use bindle_loader::{BindleLoader, ModelMetadata};

mod inference;
pub use inference::{
    ExecutionTarget,
    Graph, GraphEncoding, GraphExecutionContext, 
    TractEngine, ModelState, TensorType, InferenceEngine};

mod settings;
pub use settings::{load_settings, ModelSettings};

mod hashmap_ci;
pub (crate) use hashmap_ci::make_case_insensitive;

pub type BindlePath = String;
pub type ModelName = String;
pub type ModelZoo = HashMap<ModelName, ModelContext>;

#[derive(Clone, Debug, Default, PartialEq, Deserialize)]
pub struct ModelContext {
    pub bindle_url: BindlePath,
    pub graph_encoding: GraphEncoding,
    pub execution_target: ExecutionTarget,
    pub tensor_type: TensorType,
    pub graph_execution_context: GraphExecutionContext,
    pub graph: Graph
}

impl ModelContext {
    // /// load model data
    // pub fn load_model_data(&self, model: Vec<u8>) {}

    /// load metadata
    pub fn load_metadata(mut self, metadata: ModelMetadata) -> Result<ModelContext, Error> {

        self.graph_encoding = match metadata.graph_encoding.as_str() {
            "ONNX" => Ok(GraphEncoding(GraphEncoding::GRAPH_ENCODING_ONNX)),
            _      => Err(())
        }.map_err(|_| Error::InvalidParameter(format!("invalid 'graph_encoding'")))?;

        self.tensor_type = match metadata.tensor_type.as_str() {
            "F16" => Ok(TensorType(TensorType::F16)),
            "F32" => Ok(TensorType(TensorType::F32)),
             "U8" => Ok(TensorType(TensorType::U8)),
            "I32" => Ok(TensorType(TensorType::I32)),
               _  => Err(()),
        }.map_err(|_| Error::InvalidParameter(format!("invalid 'tensor_type'")))?;
 
        self.execution_target = match metadata.execution_target.as_str() {
            "CPU" => Ok(ExecutionTarget(ExecutionTarget::EXECUTION_TARGET_CPU)),
            "GPU" => Ok(ExecutionTarget(ExecutionTarget::EXECUTION_TARGET_GPU)),
            "TPU" => Ok(ExecutionTarget(ExecutionTarget::EXECUTION_TARGET_TPU)),
               _  => Err(()),
        }.map_err(|_| Error::Settings(format!("invalid 'execution_target'")))?;


        Ok(self)
    }

    // pub fn somewhat(){
    //     TractEngine.load(builder: &GraphBuilder, encoding: GraphEncoding, target: ExecutionTarget)
    // }
}

/// generates an error default ResultStatus
pub fn get_result_status(ml_error_option: Option<MlError>) -> ResultStatus {
    let with_error = ml_error_option.is_some();
    
    ResultStatus {
        has_error: with_error,
        error: ml_error_option
    }
}

/// generates an error default ResultStatus
pub fn get_default_inference_result(ml_error: Option<MlError>) -> InferenceOutput {
    InferenceOutput {
        result: get_result_status(ml_error),
        tensor: TensorOut {
            buffer_size: Some(0),
            data: vec![],
        }
    }
}

/// errors generated by this crate
#[derive(ThisError, Debug)]
pub enum Error {
    #[error("invalid parameter: {0}")]
    InvalidParameter(String),

    #[error("problem reading settings: {0}")]
    Settings(String),

    #[error("provider startup: {0}")]
    Init(String)
}
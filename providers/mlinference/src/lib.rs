use serde::Deserialize;
use std::collections::HashMap;
use thiserror::Error as ThisError;
use wasmcloud_interface_mlinference::{InferenceOutput, MlError, ResultStatus, Tensor, TensorType};

mod bindle_loader;
pub use bindle_loader::{BindleLoader, ModelMetadata};

pub mod inference;
pub use inference::{
    bytes_to_f32_vec, f32_vec_to_bytes, ExecutionTarget, Graph, GraphEncoding,
    GraphExecutionContext, InferenceEngine, TType, TractEngine,
};

mod settings;
pub use settings::{load_settings, ModelSettings};

mod hashmap_ci;
pub(crate) use hashmap_ci::make_case_insensitive;

pub type BindlePath = String;
pub type ModelName = String;
pub type ModelZoo = HashMap<ModelName, ModelContext>;

#[derive(Clone, Debug, PartialEq, Deserialize)]
pub struct ModelContext {
    pub bindle_url: BindlePath,
    pub graph_encoding: GraphEncoding,
    pub execution_target: ExecutionTarget,
    pub tensor_type: TensorType,
    pub graph_execution_context: GraphExecutionContext,
    pub graph: Graph,
}

impl ModelContext {
    pub fn default() -> ModelContext {
        ModelContext {
            bindle_url: Default::default(),
            graph_encoding: Default::default(),
            execution_target: Default::default(),
            tensor_type: TensorType::F32(0),
            graph_execution_context: Default::default(),
            graph: Default::default(),
        }
    }
    

    /// load metadata
    pub fn load_metadata(&mut self, metadata: ModelMetadata) -> Result<&ModelContext, Error> {
        self.graph_encoding = metadata.graph_encoding;

        self.tensor_type = match metadata.tensor_type.as_str() {
            "F16" => Ok(TensorType::F16(0)),
            "F32" => Ok(TensorType::F32(0)),
            "U8"  => Ok(TensorType::U8(0)),
            "I32" => Ok(TensorType::I32(0)),
            _ => Err(()),
        }
        .map_err(|_| Error::InvalidParameter("invalid 'tensor_type'".to_string()))?;

        self.execution_target = metadata.execution_target;

        Ok(self)
    }
}

/// generates an error default ResultStatus
pub fn get_result_status(ml_error_option: Option<MlError>) -> ResultStatus {
    let with_error = ml_error_option.is_some();

    ResultStatus {
        has_error: with_error,
        error: ml_error_option,
    }
}

/// generates an error default ResultStatus
pub fn get_default_inference_result(ml_error: Option<MlError>) -> InferenceOutput {
    InferenceOutput {
        result: get_result_status(ml_error),
        tensor: Tensor {
            tensor_type: TensorType::F32(0),
            dimensions: vec![],
            data: vec![],
        },
    }
}

/// errors generated by this crate
#[derive(ThisError, Debug)]
pub enum Error {
    #[error("invalid parameter: {0}")]
    InvalidParameter(String),

    #[error("problem reading settings: {0}")]
    Settings(String),

    #[error("provider startup: {0}")]
    Init(String),
}
